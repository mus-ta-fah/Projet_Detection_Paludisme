# backend/app/ml/model_manager.py
# ========================================
# MODEL MANAGER - Gestion des Mod√®les ML
# ========================================

import os
import yaml
import numpy as np
from typing import Dict, List, Optional
from app.ml.model_loader import load_model as remote_load_model, download_model_if_needed
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class ModelManager:
    """
    Gestionnaire centralis√© pour tous les mod√®les ML
    Permet de charger, changer et comparer les mod√®les
    """
    
    def __init__(self, config_path: str = "app/ml/model_config.yaml"):
        self.config_path = config_path
        self.models: Dict[str, any] = {}
        self.config: Dict = {}
        self.current_model_id: str = "model_2"  # D√©faut
        
        self._load_config()
        self._load_models()
    
    def _load_config(self):
        """Charge la configuration des mod√®les"""
        try:
            with open(self.config_path, 'r') as f:
                self.config = yaml.safe_load(f)
            logger.info(f"‚úÖ Configuration charg√©e: {len(self.config['models'])} mod√®les")
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement config: {e}")
            raise
    
    def _load_models(self):
        """Charge tous les mod√®les en lazy loading"""
        for model_config in self.config['models']:
            model_id = model_config['id']
            local_path = model_config['local_path']

            self.models[model_id] = {
                'path': local_path,
                'loaded': False,
                'model': None,
                'config': model_config
            }

            logger.info(f"üìÅ Mod√®le {model_id} enregistr√© (lazy loading)")
    
    def load_model(self, model_id: str):
        if model_id not in self.models:
            raise ValueError(f"Mod√®le {model_id} non trouv√©")
        
        model_info = self.models[model_id]

        if not model_info['loaded']:
            try:
                # T√©l√©chargement si n√©cessaire + chargement
                local_path = download_model_if_needed(model_info['config'])
                model_info['model'] = remote_load_model(model_info['config'])
                model_info['loaded'] = True
                logger.info(f"‚úÖ Mod√®le {model_id} charg√© depuis {local_path}")
            
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement mod√®le {model_id}: {e}")
                raise

        return model_info['model']

    
    def predict(self, 
                image: np.ndarray, 
                model_id: Optional[str] = None,
                return_all: bool = False) -> Dict:
        """
        Fait une pr√©diction avec le mod√®le sp√©cifi√©
        
        Args:
            image: Image pr√©trait√©e (64x64x3)
            model_id: ID du mod√®le √† utiliser (None = d√©faut)
            return_all: Si True, retourne pr√©dictions de tous les mod√®les
        
        Returns:
            Dict avec r√©sultats de pr√©diction
        """
        if return_all:
            return self._predict_all_models(image)
        
        # Utiliser le mod√®le sp√©cifi√© ou le mod√®le par d√©faut
        model_id = model_id or self.current_model_id
        model = self.load_model(model_id)
        model_config = self.models[model_id]['config']
        
        # Pr√©diction
        prediction_proba = model.predict(image, verbose=0)[0][0]
        
        is_parasitized = prediction_proba > 0.5
        confidence = prediction_proba if is_parasitized else (1 - prediction_proba)
        
        return {
            'model_id': model_id,
            'model_name': model_config['name'],
            'prediction': 'Parasit√©e' if is_parasitized else 'Non infect√©e',
            'confidence': float(confidence * 100),
            'probability_parasitized': float(prediction_proba * 100),
            'probability_uninfected': float((1 - prediction_proba) * 100),
            'is_parasitized': bool(is_parasitized),
            'inference_time_ms': model_config.get('inference_time_ms', 0),
            'accuracy': model_config.get('accuracy', 0)
        }
    
    def _predict_all_models(self, image: np.ndarray) -> Dict:
        """Fait une pr√©diction avec tous les mod√®les"""
        results = {
            'predictions': [],
            'ensemble_result': None
        }
        
        probas = []
        
        for model_id in self.models.keys():
            try:
                result = self.predict(image, model_id=model_id)
                results['predictions'].append(result)
                probas.append(result['probability_parasitized'] / 100)
            except Exception as e:
                logger.error(f"Erreur pr√©diction {model_id}: {e}")
        
        # Calcul ensemble (moyenne pond√©r√©e)
        if probas:
            # Pond√©ration selon l'accuracy
            weights = [m['config']['accuracy'] for m in self.models.values()]
            total_weight = sum(weights)
            weights = [w / total_weight for w in weights]
            
            ensemble_proba = sum(p * w for p, w in zip(probas, weights))
            is_parasitized = ensemble_proba > 0.5
            confidence = ensemble_proba if is_parasitized else (1 - ensemble_proba)
            
            results['ensemble_result'] = {
                'model_id': 'ensemble',
                'model_name': 'Ensemble (Weighted Average)',
                'prediction': 'Parasit√©e' if is_parasitized else 'Non infect√©e',
                'confidence': float(confidence * 100),
                'probability_parasitized': float(ensemble_proba * 100),
                'probability_uninfected': float((1 - ensemble_proba) * 100),
                'is_parasitized': bool(is_parasitized),
                'weights': weights
            }
        
        return results
    
    def get_models_info(self) -> List[Dict]:
        """Retourne les infos de tous les mod√®les"""
        return [
            {
                'id': model_id,
                'name': info['config']['name'],
                'accuracy': info['config']['accuracy'],
                'inference_time_ms': info['config'].get('inference_time_ms', 0),
                'parameters': info['config'].get('parameters', 'N/A'),
                'use_case': info['config'].get('use_case', ''),
                'is_default': info['config'].get('is_default', False),
                'loaded': info.get('loaded', False)
            }
            for model_id, info in self.models.items()
        ]
    
    def set_default_model(self, model_id: str):
        """D√©finit le mod√®le par d√©faut"""
        if model_id not in self.models:
            raise ValueError(f"Mod√®le {model_id} non trouv√©")
        
        self.current_model_id = model_id
        logger.info(f"‚úÖ Mod√®le par d√©faut: {model_id}")
    
    def compare_models(self, image: np.ndarray) -> Dict:
        """
        Compare les performances de tous les mod√®les sur une image
        Utile pour la page de comparaison
        """
        results = self._predict_all_models(image)
        
        comparison = {
            'image_analyzed': True,
            'models_compared': len(results['predictions']),
            'predictions': results['predictions'],
            'ensemble': results['ensemble_result'],
            'consensus': self._calculate_consensus(results['predictions']),
            'disagreements': self._find_disagreements(results['predictions'])
        }
        
        return comparison
    
    def _calculate_consensus(self, predictions: List[Dict]) -> Dict:
        """Calcule le consensus entre les mod√®les"""
        parasitized_count = sum(1 for p in predictions if p['is_parasitized'])
        total = len(predictions)
        
        return {
            'majority_vote': 'Parasit√©e' if parasitized_count > total / 2 else 'Non infect√©e',
            'agreement_percentage': (max(parasitized_count, total - parasitized_count) / total) * 100,
            'unanimous': parasitized_count == 0 or parasitized_count == total
        }
    
    def _find_disagreements(self, predictions: List[Dict]) -> List[Dict]:
        """Trouve les d√©saccords entre mod√®les"""
        disagreements = []
        
        for i, pred1 in enumerate(predictions):
            for pred2 in predictions[i+1:]:
                if pred1['is_parasitized'] != pred2['is_parasitized']:
                    disagreements.append({
                        'model_1': pred1['model_name'],
                        'model_2': pred2['model_name'],
                        'difference': abs(pred1['confidence'] - pred2['confidence'])
                    })
        
        return disagreements
    
    def benchmark_models(self) -> Dict:
        """
        Benchmark de tous les mod√®les
        (√Ä utiliser avec un dataset de test)
        """
        return {
            'models': self.get_models_info(),
            'recommendation': self._get_recommendation()
        }
    
    def _get_recommendation(self) -> Dict:
        """Recommande le meilleur mod√®le selon le cas d'usage"""
        return {
            'speed': 'model_1',  # Le plus rapide
            'balanced': 'model_3',  # Le plus pr√©cis
            'accuracy': 'model_2',  # √âquilibr√©
            'production': 'model_2',  # Recommand√© pour production
            'ensemble': 'ensemble'  # Maximum de pr√©cision
        }


# ========================================
# EXEMPLE D'UTILISATION
# ========================================

if __name__ == "__main__":
    # Initialiser le gestionnaire
    manager = ModelManager()
    
    # Obtenir infos sur tous les mod√®les
    print("üìä Mod√®les disponibles:")
    for model in manager.get_models_info():
        print(f"  - {model['name']}: {model['accuracy']*100:.2f}% accuracy")
    
    # Exemple de pr√©diction (avec image fictive)
    # image = np.random.rand(1, 64, 64, 3)
    
    # Pr√©diction avec mod√®le par d√©faut
    # result = manager.predict(image)
    
    # Pr√©diction avec mod√®le sp√©cifique
    # result = manager.predict(image, model_id='model_1')
    
    # Pr√©diction avec tous les mod√®les
    # results = manager.predict(image, return_all=True)
    
    # Comparaison des mod√®les
    # comparison = manager.compare_models(image)